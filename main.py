import sys

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.porter import *
from textblob import Word
from textblob import TextBlob

import spacy

from spacy.lang.en.examples import sentences

## import treetaggerwrapper    # Не импортируется. Это проблема !


# from gensim.utils import lemmatize # Не импортируется.
# Да и хрен с ним...
# И вообще, Gensim only ever previously wrapped the lemmatization routines
# of another library (Pattern) – which was not a particularly modern/maintained
# option, so was removed from Gensim-4.0.
#
# Users should choose & apply their own lemmatization operations, if any,
# as a preprocessing step before applying Gensim's algorithms.
# Some Python libraries offering lemmatization include:
#
# Pattern (Gensim's previously-included option): https://github.com/clips/pattern
# NLTK: https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer
# UDPipe: https://ufal.mff.cuni.cz/udpipe
# Spacy: https://spacy.io/api/lemmatizer
# Stanza: https://stanfordnlp.github.io/stanza/
# Ну да, ну да... Gensim когда-либо ранее упаковывал подпрограммы лемматизации
# другой библиотеки (Pattern), что не было особенно современным или хорошо
# обслуживаемым вариантом, поэтому он был удален из Gensim-4.0.
# Пользователи могут выбирать и применять свои собственные операции лемматизации,
# если они считают это необходимым, в качестве этапа предварительной обработки
# перед применением алгоритмов Gensim.
#
# Некоторые библиотеки Python, предлагающие лемматизацию, включают:
# Шаблон (ранее включенная опция Gensim, которую можно использовать напрямую, а не через старую поддержку Gensim): https: // github .com / clips / pattern
# NLTK: https://www.nltk.org/ api / nltk.stem.html # nltk.stem.wordnet.WordNetLemmatizer
# UDPipe: https://ufal.mff.cuni.cz/udpipe
# Просторный: https://spacy.io/api/lemmatizer
# Станца: https://stanfordnlp.github.io/stanza/

"""
Предварительная обработка текстовых данных в Python для NLP:
Задача – переход от того, что называется куском текста 
(не путать с фрагментацией текста !!!), одной длинной необработанной строки, 
и получить список (или несколько списков) чистых лексем, которые будут полезны 
для дополнительных задач текстового анализа и/или обработки естественного языка.

Задачи текстового анализа и/или обработки естественного языка
Извлечение смысла
Процесс чтения и понимания текста сам по себе сложен. Кроме того, люди часто не соблюдают логику
и последовательность повествования. Например, что может значить вот этот заголовок новостей?

Environmental regulators grill business owner over illegal coal fires.

Регуляторы допрашивают владельца бизнеса о незаконном сжигании угля? 
А может быть, они в буквальном смысле готовят его на гриле? 
Человек догадался, а сможет ли компьютер?

Реализация какой-либо сложной комплексной задачи в машинном обучении 
обычно означает построение пайплайна (конвейера). 
Смысл этого подхода в том, чтобы разбить проблему на очень маленькие 
части и решать их отдельно. Соединив несколько таких пайплайнов (моделей), 
поставляющих друг другу данные, можете получать замечательные результаты.

Именно эта стратегия будет использована для примера. 
Cначала нужно разбить процесс языкового анализа на стадии и понять, как они работают.

NLP пайплайн шаг за шагом
В качестве примера предлагается отрывок, взятый из Википедии:

London is the capital and most populous city of England and the United Kingdom. 
Standing on the River Thames in the south east of the island of Great Britain, 
London has been a major settlement for two millennia. 
It was founded by the Romans, who named it Londinium.

В этом параграфе содержится несколько полезных фактов. 
Задача: сделать так, чтобы компьютер смог понять, 
что Лондон – это город, 
что он расположен в Англии, 
был основан римлянами и т. д. 
Но прежде всего его надо научить базовым концепциям письменного(!!!) языка.

Шаг 1. Выделение предложений
Первый этап пайплайна – разбить текст на отдельные предложения. 
В результате получается следующее:

= London is the capital and most populous city of England and the United Kingdom.
Standing on the River Thames in the south east of the island of Great Britain, 
= London has been a major settlement for two millennia.
= It was founded by the Romans, who named it Londinium.

Можно предположить, что каждое предложение – это самостоятельная мысль или идея. 
Проще научить программу понимать единственное предложение, а не целый параграф.
Можно было бы просто разделять текст по определенным знакам препинания. 
Но современные NLP пайплайны имеют в запасе более сложные методы, подходящие даже для работы
с неформатированными фрагментами.

Шаг 2. Токенизация, или выделение слов
Теперь можно обрабатывать полученные предложения по одному (с самого первого):

London is the capital and most populous city of England and the United Kingdom.

Следующий шаг конвейера – выделение отдельных слов или токенов – токенизация. 
Результат на этом этапе выглядит так:

«London», «is», «the», «capital», «and», «most», «populous», «city», 
«of», «England», «and», «the», «United», «Kingdom», «.»

В английском языке это просто. Фрагмент текста отделяется  каждый раз, 
когда встречается пробел. 
Знаки препинания, как и обычные слова, тоже являются токенами, 
поскольку могут иметь важное значение.

Шаг 3. Определение частей речи
Теперь для каждого токена надо угадать, какой частью речи он является: 
существительным, глаголом, прилагательным или чем-то другим. 
Зная роль каждого слова в предложении, можно понять его общий смысл.

На этом шаге анализируется каждое слово вместе с его ближайшим окружением 
с помощью предварительно подготовленной классификационной модели:

word: "London" (соседние токены: "is", "the", "capinal")
        |
определение части речи
        |
"proper_noun" (имя собственное)

Эта модель была обучена на миллионах английских предложений с уже обозначенными 
частями речи для каждого слова и теперь способна их распознавать.
Этот анализ основан на статистике (!!!) – на самом деле модель НЕ понимает смысла слов, 
вложенного в них человеком. Она просто знает, как угадать часть речи, 
основываясь на похожей структуре предложений и ранее изученных токенах.

После обработки получаем следующий результат:

  London            is      the       capital      and   most     populous          city 
имя_собственное  глагол  артикль  существительное  союз  наречие  прилагательное существительное

С этой информацией уже можно начинать анализировать смысл. Например, если обнаружены существительные
«London» и «capital», вероятно, в предложении говорится о Лондоне.

Шаг 4. Лемматизация
В английском и большинстве других языков слова могут иметь различные формы. Например:

I had a pony.
I had two ponies.

Оба предложения содержат существительное «pony», но с разными окончаниями. 
Если тексты обрабатывает компьютер, чтобы понимать, что речь идет об одной и той же концепции пони,
он должен знать основную форму каждого слова. Иначе токены «pony» и «ponies» будут восприняты 
как совершенно разные.

В NLP этот процесс называется лемматизацией – нахождением основной формы (леммы) каждого слова
в предложении. Это относится и к глаголам. Их можно привести к их неопределенной форме. 
Таким образом, предложение «I had two ponies» превращается в «I [have] two [pony]».

Лемматизация обычно выполняется простым поиском форм в таблице. 
Кроме того, можно добавить некоторые пользовательские правила для анализа слов.

Вот так выглядит предложение о Лондоне после его обработки (лемматизации):

London            is      the       capital        and   most       populous        city 
                  be
имя_собственное  глагол  артикль  существительное  союз  наречие  прилагательное существительное

Единственное изменение – превращение «is» в «be».

Шаг 5. Определение стоп-слов
Теперь нужно определить важность каждого слова в предложении. 
В английском очень много вспомогательных слов, например, «and», «the», «a». 
При статистическом анализе текста эти токены создают много шума, так как появляются чаще, 
чем остальные. Некоторые NLP пайплайны отмечают их как стоп-слова и отсеивают перед 
подсчетом количества.

Теперь предложение о Лондоне выглядит следующим образом:

London            ..      ...       capital        ...    ...       populous        city 
                  be
имя_собственное  глагол  артикль  существительное  союз  наречие  прилагательное существительное

Для обнаружения стоп-слов обычно используются готовые таблицы. Однако тут нет единого стандартного
списка, подходящего в любой ситуации. Игнорируемые токены могут меняться (!!!), все зависит 
от особенностей проекта.
Например, если нужно создать движок для поиска рок-групп, вероятно, не надо игнорировать 
артикль «the». Он встречается в названии множества групп, а одна известная группа 80-х 
даже называется «The The!».

Шаг 6. Парсинг зависимостей
Теперь необходимо установить взаимосвязь между словами в предложении. 
Это называется парсингом зависимостей. Конечная цель этого шага – построение дерева, 
в котором каждый токен имеет единственного родителя. Корнем может быть главный глагол.

                 root
                  |
London            ..      the       capital        and   most       populous        city 
                  be

имя_собственное  глагол  артикль  существительное  союз  наречие  прилагательное существительное

Но следует сделать еще кое-что. Нужно не только определить родителя, но и установить 
тип связи между двумя словами:

                 root       +---def.-+
                  |         |        | +---------------- conjunction -----------------+
                  |         |        | |                                              |
London            ..      the       capital        and   most       populous        city 
  |               be                  | |           |      |          | |            |
  |               |                   | |           |      |          | +- modyfier--+
  +--- subject ---+----- attribute ---+ |           |      |          |
                                        |           |      adv.modyfier       
                                        +---conj.---+

имя_собственное  глагол  артикль  существительное  союз  наречие  прилагательное существительное

Это дерево парсинга демонстрирует, что главный субъект предложения – это существительное «London».
Между ним и «capital» существует связь «be». Вот так можно узнать, что Лондон – это столица! 
Если проследовать дальше по веткам дерева (уже за границами схемы), то можно было бы узнать, 
что Лондон – это столица Соединенного Королевства. 
Парсинг зависимостей работает примерно так же как и процедура определения части речи с 
помощью модели машинного обучения. Модель получает слова и возвращает результат. 
Однако это более сложная задача. 
Технологии парсинга активно развиваются и постоянно улучшаются.

Также важно, что многие английские предложения неоднозначны и сложны для анализа. 
В таких случаях модель делает предположение о наиболее вероятном значении, 
хотя это не всегда получается. В будущем NLP-модели будут совершенствоваться и 
более разумно обрабатывать тексты. Для проведения парсинг зависимостей можно применить spaCy.

Шаг 6б. Поиск групп существительных
Ранее в предложении рассматривалось каждое слово как отдельная сущность. 
Но иногда имеет смысл сгруппировать токены, которые относятся к одной и той же идее или вещи. 
Чтобы автоматически объединить такие слова, можно использовать полученное дерево парсинга.

Например, вместо этого:

London            is      the       capital        and   most       populous        city 
                  be
имя_собственное  глагол  артикль  существительное  союз  наречие  прилагательное существительное

Можно получить такой результат:

London            is      the capital        and     most populous city 
                  be
имя_собственное  глагол  существительное     союз       существительное

Это необязательный шаг. Группировку можно делать или не делать в зависимости от 
конечной цели проекта. Часто это быстрый и удобный способ упростить предложение, 
если вместо максимально подробной информации о словах мы стремимся извлечь законченные идеи.

Шаг 7. Распознавание именованных сущностей (Named Entity Recognition, NER)
Уже сделана вся сложную работу, наконец-то можно перейти от школьной грамматики к 
реально интересным задачам.
В предложении присутствуют следующие существительные:

London   is  the  capital   and   most  populous  city of England and the United Kingdom .
-------           -------                         ----    -------         ---------------
Некоторые из них обозначают реальные вещи. Например, «London», «England» и «United Kingdom» – 
это точки на карте. Хорошо было бы определять их. При помощи NLP можно автоматически извлекать
список реальных объектов, упомянутых в документе.
Цель распознавания именованных сущностей – обнаружить такие существительные и связать их
с реальными концепциями. После обработки каждого токена NER-моделью предложение будет 
выглядеть вот так:

London   is  the  capital   and   most  populous  city of England and the United Kingdom .
-------                                                   -------         ---------------
географическая сущность                        географическая сущность    географическая сущность

NER-системы не просто просматривают словари. Они анализируют контекст токена в предложении и 
используют статистические модели, чтобы угадать какой объект он представляет. 
Хорошие NER-системы способны отличить актрису Brooklyn Decker от города Brooklyn.

Большинство NER-моделей распознают следующие типы объектов:

= имена людей;
= названия компаний;
= географические обозначения (и физические, и политические);
= продукты;
= даты и время;
= денежные суммы;
= события.

Так как эти модели позволяют легко извлекать из сплошного текста структурированные данные, 
они очень активно используются в разных областях. 
Это один из самых простых способов извлечения информации из NLP-конвейера.
И опять spaSy !

Шаг 8. Разрешение кореференции
Tсть отличное и полезное представление анализируемого предложения. Известно, как связаны друг 
с другом слова, к каким частям речи они относятся и какие именованные объекты обозначают.
И все-таки остаётся большая проблема. 
В английском очень много местоимений – слов вроде he, she, it. 
Это сокращения, которыми заменяются на письме настоящие имена и названия. 
Человек может проследить взаимосвязь этих слов от предложения к предложению, 
основываясь на контексте. 
Но NLP-модель не знает о том, что означают местоимения, так как она рассматривает 
всего одно предложение за раз.
Надо посмотреть на третью фразу в документе:

It was founded by the Romans, who named it Londinium.

Если ее пропустить через конвейер, то можно узнать, что «это» было основано римлянами. 
Это не очень полезное знание.
Человек легко догадается в процессе чтения, что «это» не что иное как Лондон. 
Разрешением кореференции называется отслеживание местоимений в предложениях 
с целью выбрать все слова, относящиеся к одной сущности.
Вот результат обработки документа для слова «London»:

London is the capital and most populous city of England and the United Kingdom. 
------
Standing on the River Thames in the south east of the island of Great Britain, 
London has been a major settlement for two millennia. 
------
It was founded by the Romans, who named it Londinium.
--

Скомбинировав эту методику с деревом парсинга и информацией об именованных сущностях,
можно получить возможность извлечь из документа огромное количество полезных данных.
Разрешение кореференции – один из самых трудных шагов в пайплайне. 
Он даже сложнее парсинга предложений. В области глубокого обучения уже появились 
способы его реализации, они достаточно точны, но все еще не совершенны. 

Пайплан NLP
Cхема конвейера:

сегментация предложений
    токенизация
        определение частей речи
            лемматизация
                стоп-слова
                    парсинг зависимостей
                        группы существительных
                            именованные сущности
                                разрешение кореференции  

Это стандартные этапы обычного NLP-конвейера, но в зависимости от конечной цели проекта и
особенностей реализации модели, некоторые из них можно пропускать или менять местами. 
Например, spaCy производит сегментацию позже, используя для нее результаты парсинга зависимостей.

И как же запрограммировать этот пайплайн? Хорошая новость. Это уже сделано за нас (!!!) 
в библиотеках языка Python! Все перечисленные шаги уже написаны и готовы к использованию.

Извлечение фактов
spaCy обладает большим объёмом базовых возможностей. Но spaCy также можно использовать 
для подготовки данных, которые затем будут обрабатываться более сложными алгоритмами извлечения. 
Например, библиотекой textacy, которая хорошо работает поверх spaCy.

Проанализировав дерево парсинга с помощью алгоритма извлечения полуструктурированных выражений, 
можно найти простые сочетания со словом Лондон и различными формами глагола “быть”. 

Библиотеки
используются следующие библиотеки:

NLTK : Natural Language Toolkit – одна из самых популярных и широко используемых 
       NLP библиотек в Python, полезная для всего, начиная от токенизации, 
       кончая стеммингом, тегированием части речи и т.д.
BeautifulSoup : BeautifulSoup – это библиотека для извлечения данных из документов HTML и XML.
Inflect: простая библиотека для выполнения задач, связанных с естественным языком, 
         по созданию множественного числа, единственного числа существительных и (самое главное) 
         преобразованию чисел в слова.
Contractions: библиотека, предназначенная только для расширения сокращений.
num2words: библиотека, предназначенная для преобразования цифр в слова.

Если эти библиотеки не установлены, можно это сделать командами:

pip install nltk
pip install contractions
pip install inflect
pip install beautifulsoup
pip install num2words

Базовые понятия
Устранение шума - специфическая задача нормализации текста, 
которая часто выполняется перед токенизацией. 
Если два других основных этапа препроцессинга (токенизация и нормализация) 
в основном не зависят от задачи, то этот процесс в большей степени зависит 
от конкретной задачи.

Типовые задачи по устранению шума:

= удалить верхние и нижние регистры текстового файла
= удалить разметку и метаданные HTML, XML и т.д.
= извлечение важных данных из других форматов, таких как JSON

Граница между устранением шума и сбором и обработкой данных, с одной стороны, нечеткая. 
Учитывая их тесную связь с конкретными текстами, их сбором и обработкой, многие подобные задачи, 
такие как разбор структуры JSON, очевидно, должны быть реализованы до токенизации.

В механизме предварительной обработки данных HTML-разметка удаляется с помощью библиотеки 
BeautifulSoup, удаление открытых и закрытых двойных скобок и любых промежуточных элементов
производится с помощью регулярных выражений.
Замена сокращений на их расширения, так как токенизатор разделяет слова типа 
“не мог” на “не” и “мог”.  

Токенизация
Токенизация – это шаг, который разбивает длинные строки текста на более мелкие фрагменты 
или лексемы. Большие куски текста могут быть преобразованы в предложения, 
предложения – в слова и т.д. Дальнейшая обработка обычно выполняется после того, 
как кусок текста был правильно скомпонован. 
Токенизация также известна как сегментация текста или лексический анализ.
После токенизации работа ведётся уже не на уровне текста, а на уровне слова. 

Сегментация иногда используется для обозначения разбиения большого куска текста на части,
превышающие по размеру слова (например, абзацы или предложения), в то время как 
токенизация предназначена для процесса разбиения, исключительно на слова.

Нормализация
Нормализация – это метод, при котором набор слов в предложении преобразуется в последовательность, 
чтобы сократить время поиска. Слова, которые имеют то же значение, но имеют некоторые различия в 
зависимости от контекста или предложения, нормализуются.
Другими словами, есть одно корневое слово, но есть много вариантов одних и тех же слов. 
Например, корневое слово «есть» и его вариации «есть, есть и так далее». 
Под нормализацией понимается ряд связанных задач, направленных на то, чтобы поставить
весь текст в равные условия: преобразование всего текста в один регистр (верхний или нижний), 
удаление пунктуации, преобразование чисел в их словесные эквиваленты и так далее. 
Нормализация ставит все слова в равные условия и позволяет обрабатывать их одинаково.

Нормализация текста может подразумевать выполнение ряда задач, 
но обычно нормализация предполагает три отдельных этапа: 
(1) деривация, (2) лемматизация и (3) все остальное.

Лемма - это базовая или словарная форма слова. 
Лемматизатор - процедура, которая (старается ???) вернуть лемму (словарную форму слова).
Для этого он рассматривает весь (доступный лематизатору) словарный запас языка, 
чтобы применить морфологический анализ к словам.
Она стремится удалить флективные окончания, старается выйти за "рамки"
сокращения слов.

Стемминг 
Стемминг это своего рода нормализация слов. 
Точно так же, с помощью Stemming, можно найти корневое слово любых вариаций.
В различных предложениях естественного языка может быть одно и то же значение.
(То есть верховая (???) активность в прошлом. 
Человек может легко понять, что оба значения одинаковы. 
Но для машин оба предложения разные. В случае, если НЕ предоставляется один и тот же набор данных, 
машина не может предсказать тождества предложений. 
Трудно преобразовать предлоения в одну строку данных. Поэтому необходимо 
дифференцировать значение каждого слова, чтобы подготовить набор данных для 
машинного обучения. Здесь и применяется stemming для классификации данных того же типа 
путем получения их корневого слова.
Stemming – это модуль предварительной обработки данных. 
В английском есть много вариантов одного слова. Эти вариации создают неоднозначность
в обучении и прогнозировании машинного обучения. Чтобы создать успешную модель, 
важно отфильтровать такие слова и преобразовать их в последовательные данные того же типа, 
используя stemming. Кроме того, это важная техника для получения данных строки 
из набора предложений и удаления избыточных данных, также известных как нормализация.

Лемматизация – это алгоритмический процесс нахождения леммы слова в зависимости
от его значения. Лемматизация обычно относится к морфологическому анализу слов, 
целью которого является удаление флективных окончаний. 
Это помогает в возвращении базовой или словарной формы слова, которое известно как лемма. 
Метод лемматизации NLTK основан на встроенной морф-функции WorldNet. 
Предварительная обработка текста включает в себя как основы, так и лемматизации. 
И это не одно и том же. Между ними есть разница. 
Лемматизация предпочтительнее основ (???) по следующей причине.
Почему лемматизация лучше стемминга?
Алгоритм Stemming работает, вырезая суффикс из слова. В более широком смысле отсекает 
начало или конец слова.
Наоборот, лемматизация является более мощной операцией и учитывает морфологический 
анализ слов. Она возвращает лемму, которая является базовой формой всех ее флективных форм.
Для создания словарей и поиска правильной формы слова необходимы глубокие лингвистические знания(!!!).
Стемминг – это общая операция, а лемматизация – интеллектуальная операция, 
в которой правильная форма будет выглядеть в словаре. Лемматизация помогает в формировании 
лучших возможностей машинного обучения.
Лемматизатор минимизирует неоднозначность текста. 
Примеры слов, таких как велосипед или велосипеды, преобразуются в базовое слово велосипед. 
Лемматизатор преобразует все слова, имеющие одинаковое значение, но разное представление,
в их базовую форму. Это уменьшает плотность слова в данном тексте и помогает в подготовке 
точных характеристик для учебного автомата. Чем чище данные, тем интеллектуальнее и точнее 
будет модель машинного обучения. Лемматизатор также экономит память и вычислительные затраты.

Далее рассматриваются реализации лемматизации с помощью следующих пакетов Python.

= Wordnet Lemmatizer        --> Лемматизатор Wordnet из NLTK
                                Wordnet – это большая, свободно распространяемая
                                и общедоступная лексическая база данных для 
                                английского языка с целью установления структурированных 
                                семантических отношений между словами. 
                                Библиотека также предлагает возможности лемматизации и является
                                одним из самых ранних и наиболее часто используемых лемматизаторов.

= TextBlob                  --> TextBlob Lemmatizer – это мощный, быстрый пакет NLP. 
                                Используя объекты Word и TextBlob, довольно просто 
                                анализировать и лемматизировать слова и предложения 
                                соответственно. Чтобы лемматизировать предложение или абзац, 
                                он анализируется его с помощью TextBlob а затем вызывается функция
                                lemmatize() для проанализированных слов. Он не справляетчя с работой. 
                                Как и NLTK, TextBlob внутри использует Wordnet. Соотвественно, 
                                для корректной работы он так же требует передачу соответствующего 
                                POS-тега методу lemmatize(). TextBlob Lemmatizer примнняется
                                с соответствующим
                                POS-тегом

= spaCy Lemmatizer         --> spaCy лемматизация является относительно новым пакетом
                               и на данный момент считается стандартом в индустрии NLP.
                               Он поставляется с предварительно созданными моделями, 
                               которые могут анализировать текст и выполнять различный 
                               функционал, связанный с NLP. 

= CLiPS Pattern             --> Pattern by CLiPs (Pattern Lemmatizer) – это универсальный
                                пакет со многими возможностями NLP. позволяет просмотреть 
                                возможные лексемы для каждого слова. Также можно получить 
                                лемму, анализируя текст.

= Stanford CoreNLP          --> Stanford CoreNLP – так же популярный (!!!) инструмент NLP, 
                                который изначально был реализован на Java. Это для Mac...
                                Также был сделано несколько враперов на Python. 
                                Он достаточно удобен (???) в использовании. Короче, НЕТ!

= Gensim Lemmatizer         --> Gensim Лемматизация. Предоставляет средства лемматизации 
                                на основе пакета pattern. Использует метод lemmatize() 
                                в модуле utils. По умолчанию Gensim lemmatize() 
                                допускает только теги «JJ», «VB», «NN» и «RB».

= TreeTagger                --> TreeTagger является POS таггером для многих языков. 
                                И он также предоставляем возможности лемматизации.
                                реализации НЕ будет...


__________________________________________________________________________
1. WordNet -> лексическая база данных из более чем 200 языков.
Обеспечивает семантические отношения между словами. 
Он присутствует в библиотеке nltk на python.
Wordnet связывает слова в семантические отношения (например, синонимы).
Он группирует синонимы в виде синтаксических наборов.
synsets : группа элементов данных, которые семантически эквивалентны. 
Один из самых ранних и наиболее часто используемых методов лемматизации.

Как использовать: 
- надо загрузить пакет nltk,
- импортировать nltk:
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

__________________________________________________________________________
2. Wordnet (с тегом POS) 
В приведенном выше подходе результаты Wordnet не всегда корректны. 
Такие слова, как "сидеть", "летать" и т. д., остались прежними после лемматизации. 
Это потому, что эти слова рассматриваются как существительное в данном предложении, а не глагол. 
Чтобы преодолеть это, мы используются теги POS (часть речи).
Этот тег добавляется с определенным словом, определяющим его тип 
(глагол, существительное, прилагательное и т. д.).
Например,
Word + Type (POS—тег) -> Лемматизированное слово
вождение + глагол 'v' —> водить
собак + существительное 'n' —> собака

--------------------------------------------------------------------------
3. Текстовый блок TextBlob
Это библиотека python, используемая для обработки текстовых данных. 
Он предоставляет простой API для доступа к своим методам и выполнения основных задач NLP.
Надо загрузить пакет TextBlob

--------------------------------------------------------------------------
4. Текстовый блок TextPOS
То же, что и в подходе Wordnet без использования соответствующих тегов POS, 
наблюдаются те же ограничения, что и в этом подходе. 
Чтобы преодолеть эту проблему, здесь используется один из наиболее мощных 
аспектов модуля TextBlob - пометка "Часть речи".

--------------------------------------------------------------------------

5. spaCy
Работа с библиотекой spaCy для выполнения еще нескольких основных задач НЛП, 
таких как токенизация , стемминг и лемматизация.
spaCy является относительно новым пакетом и на данный момент считается 
стандартом в индустрии NLP. Он поставляется с предварительно созданными моделями, 
которые могут анализировать текст и выполнять различный функционал, связанный с NLP.
spaCy по умолчанию определяет часть речи и назначает соответствующую лемму.

Библиотека spaCy - одна из самых применяемых библиотек NLP наряду с библиотекой NLTK. 
Основное различие между двумя библиотеками заключается в том, что NLTK содержит МНОГО 
алгоритмов для решения одной проблемы, а spaCy содержит только один, но ЛУЧШИЙ алгоритм
для решения проблемы.

NLTK был выпущен еще в 2001 году, spaCy был разработан в 2015 году. 
Далее при описании NLP основном будет применяться spaCy из-за его современного состояния. 
Но когда легче выполнить задачу, используя NLTK, а не spaCy, будет применяться NLTK.

Для работы с spaCy прежде всего (!!!) надо установить spaCy и загрузить модель «en».

# Install spaCy (run in terminal/prompt)
import sys

# Установка spaCy 
pip3 install -U spacy

# Загрузка модели для анализа английского языка
### python3 -m spacy download en_core_web_lg
python -m spacy download en 

# Установка textacy
pip3 install -U textacy

https://spacy.io/models/en
Это ЕДИНСТВЕННЫЙ (!!!) сайт, с которого удалось загрузить требуемые 
компоненты spaCy

После загрузки и установки spaCy следующим шагом будет загрузка языковой модели. 
Можно использовать одновременно множество моделей (англоязычную и русскоязычную) модель. 
Языковые модели используются для выполнения множества задач НЛП.

paCy, давайте вкратце посмотрим, как с ним работать.

Первый шаг - импортирт spacy библиотеку следующим образом:

import spacy 

Далее нужно загрузить языковую модель spaCy.

 sp = spacy.load('en_core_web_sm') 

Здесь используется функция load spacy библиотеки для загрузки базовой 
английской языковой модели. Модель хранится в переменной sp
Теперь создаётся документ и используется эта модель. 
Документ может быть предложением или группой предложений и может иметь неограниченную длину. 
Следующий скрипт создает простой документ spaCy.

sentence = sp(u'Manchester United is looking to sign a forward for $90 million') 

Когда документ создается с использованием модели, SpaCy автоматически разбивает входную строку
на токены. Токен просто относится к отдельной части предложения, имеющей некоторое 
семантическое значение. 

Результат выполнения сценария выше выглядит следующим образом:

 Manchester 
 United 
 is 
 looking 
 to 
 sign 
 a 
 forward 
 for 
 $ 
 90 
 million 

Таким образом, в документе есть следующие токены. 
Также, используя .pos_ можно увидеть части речи каждого из этих токенов:

 for word in sentence: 
 print(word.text, word.pos_) 

Вывод:

 Manchester PROPN 
 United PROPN 
 is VERB 
 looking VERB 
 to PART 
 sign VERB 
 a DET 
 forward NOUN 
 for ADP 
 $ SYM 
 90 NUM 
 million NUM 

Здесь каждому слову или символу в предложении была сопоставлена часть речи. 
Например, «Манчестер» был помечен как существительное собственное, 
«Смотрю» - как глагол и так далее.

Наконец, помимо частей речи, также можено видеть зависимости.

Ещё один документ:

 sentence2 = sp(u"Manchester United isn't looking to sign any forward.") 

Для синтаксического анализа зависимостей используется атрибут dep_ :

 for word in sentence2: 
 print(word.text, word.pos_, word.dep_) 

Результат:

 Manchester PROPN compound 
 United PROPN nsubj 
 is VERB aux 
 n't ADV neg 
 looking VERB ROOT 
 to PART aux 
 sign VERB xcomp 
 any DET advmod 
 forward ADV advmod 
 . PUNCT punct 

Из выходных данных видно, что spaCy может найти 
зависимость между токенами. Например, в данном предложении
есть слово is'nt. 
Синтаксический анализатор зависимостей разбил его на два слова и указывает, 
что n't на самом деле является отрицанием предыдущего слова.

Помимо печати слов, также можно распечатать предложения из документа.

document = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?') 

Используя следующий сценарий, можно перебирать каждое предложение:

 for sentence in document.sents: 
 print(sentence) 

Результат:

 Hello from Stackabuse. 
 The site with the best Python Tutorials. 
 What are you looking for? 

Можно проверить, начинается ли предложение с определенного токена или нет. 
Можно получить отдельные токены, используя индекс и квадратные скобки, как у массива:

document[4] 

В приведенном выше сценарии в документе ищется 5-е слово 
(здесь индекс начинается с нуля, а период (???) считается токеном). 
На выходе вы должно быть:

 The 

Теперь, чтобы увидеть, начинается ли какое-либо предложение в документе с The, 
можно использовать is_sent_start:

 document[4].is_sent_start 

Поскольку токен The используется в начале второго предложения, результатом будет True.

Выше были показаны несколько основных операций библиотеки spaCy. 
Далее будут рассмотрены токенизация, 
                            стемминг и 
                                лемматизация.

Токенизация
Токенизация - это процесс разбиения документа на слова, знаки препинания, числовые цифры и т. Д.

Токенизация spaCy как она есть. С использованием следующего скрипта создаётся новый документ:

 sentence3 = sp(u'"They are leaving UK for USA"') 
 print(sentence3) 

Предложение содержит кавычки в начале и в конце. 
Оно также содержит знаки препинания в сокращениях «UK» и «USA» (где ???).
Вот как spaCy токенизует это предложение.

 for word in sentence3: 
 print(word.text) 

Вывод:

 " 
 They 
 're 
 leaving 
 UK 
 for 
 USA 
 " 
В выходных данных можро увидеть, что spaCy разметила начальную и конечную двойные кавычки. 
Тем не менее, он не разметил точку пунктуации между аббревиатурами, такими как UK и USA.
Посмотрим еще один пример токенизации:

 sentence4 = sp(u"Hello, I am non-vegetarian, email me the menu at [email protected] ") 
 print(sentence4) 

В этом предложении есть тире в слове «не-вегетарианец» и в адресе электронной почты (где ???). 
Результат токениззации spaCy:

 for word in sentence4: 
 print(word.text) 

Вывод:

 Hello 
 , 
 I 
 am 
 non 
 - 
 vegetarian 
 , 
 email 
 me 
 the 
 menu 
 at 
 [email protected] 

Видно, что spaCy смог обнаружить электронное письмо и не токенизировал его, 
несмотря на наличие знака «-» (???). 
С другой стороны, слово «не-вегетарианец» было символическим.

Теперь о подсчёте слов в документе:

 len(sentence4) 

На выходе 14 - это количество токенов в sentence4 .

Обнаружение сущностей
Помимо токенизации документов в слова, можно также  узнать, 
является ли слово сущностью, такой как компания, место, здание, валюта, учреждение и т. Д.

Пример распознавания именованных сущностей:

 sentence5 = sp(u'Manchester United is looking to sign Harry Kane for $90 million') 

Попытка простой токенизации:

 for word in sentence5: 
 print(word.text) 

Вывод:

 Manchester 
 United 
 is 
 looking 
 to 
 sign 
 Harry 
 Kane 
 for 
 $ 
 90 
 million 

Известно, что «Манчестер Юнайтед» - это одно слово, поэтому его нельзя 
токенизировать двумя словами. 
Точно так же «Гарри Кейн» - это имя человека, а «90 миллионов долларов» - 
это денежная ценность. Их также не следует токенизировать.

Здесь начинает работать распознавание именованных сущностей. 
Чтобы получить именованные сущности из документа, надо использовать атрибут ents.
Извлечение названных сущностей из приведенного выше предложения. 
Выполняется следующий скрипт:

 for entity in sentence.ents: 
 print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_))) 

В приведенном выше скрипте печатается текст объекта, 
метка объекта и детали объекта. 

Вывод:

 Manchester United - ORG - Companies, agencies, institutions, etc. 
 Harry Kane - PERSON - People, including fictional 
 $90 million - MONEY - Monetary values, including unit 

Таким образом, средство распознавания именованных сущностей spaCy успешно распознало 
«Манчестер Юнайтед» как организацию, «Гарри Кейна» как личность и «90 миллионов долларов» 
как денежную ценность.

Обнаружение существительных
Помимо обнаружения именованных сущностей, также могут быть обнаружены существительные. 
Для этого  применяется атрибут noun_chunks. Следующее предложение:

 sentence5 = sp(u'Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million') 

Поиск существительных из этого предложения:

 for noun in sentence5.noun_chunks: 
 print(noun.text)

Вывод:

 Latest Rumours 
 Manchester United 
 Harry Kane 

Видно, что существительное также может быть именованным объектом, и наоборот.

Стемминг
Основание относится к приведению слова к его корневой форме. 
При выполнении задач обработки естественного языка могутприменяться различные сценарии, 
в которых находятся разные слова с одним и тем же корнем. 
Например, compute, computer, computing, computed и т. Д. 
Для достижения единообразия можно уменьшить слова до их корневой формы. 
здесь применяется стемминг.
В spaCy нет (!!!) функции для стемминга, поскольку он полагается только на лемматизацию. 
Поэтому здесь для стемминга используется NLTK.
В NLTK есть два типа стеммеров: Porter Stemmer и Snowball . 
Оба они были реализованы с использованием разных алгоритмов.

Портер Стеммер
Стеммер портера в действии:

 import nltk 

 from nltk.stem.porter import * 

Создаётся класс PorterStemmer.

 stemmer = PorterStemmer()

Прусть имеется следующий список, и надо сократить эти слова до основы:

 tokens = ['compute', 'computer', 'computed', 'computing'] 

Следующий скрипт находит основу для слов в списке с помощью стеммера портера:

 for token in tokens: 
 print(token + ' --> ' + stemmer.stem(token)) 

Результат выглядит следующим образом:

 compute --> comput 
 computer --> comput 
 computed --> comput 
 computing --> comput 

Все 4 слова были сокращены до «вычислить», что на самом деле вовсе не слово.

"Снежок" Стеммер
Стеммер Snowball - это немного улучшенная версия стеммера Porter, 
которую обычно предпочитают последнему. Стеммер снежка в действии:

 from nltk.stem.snowball import SnowballStemmer 

 stemmer = SnowballStemmer(language='english') 

 tokens = ['compute', 'computer', 'computed', 'computing'] 

 for token in tokens: 
 print(token + ' --> ' + stemmer.stem(token)) 

В приведенном выше сценарии используется стеммер Snowball, чтобы найти основу тех же 4 слов, 
что и стеммер портера. 
Результат:

 compute --> comput 
 computer --> comput 
 computed --> comput 
 computing --> comput 

Это те же самые результаты. В качестве основы всё ещё существует «вычислитель» . 
Опять же, слово «вычислить» на самом деле не словарное.

Вот тут-то и нужна лемматизация. 
Лемматизация сокращает слово до основы, как оно появляется в словаре. 
Основы, возвращаемые посредством лемматизации, являются фактическими словарными словами 
и семантически полными, в отличие от слов, возвращаемых стеммером.

Лемматизация
Хотя и не получается выполнить стемминг с помощью spaCy, можно выполнить 
лемматизацию с помощью spaCy.

Для этого нужно использовать lemma_ атрибут на Spacy документа. 
Пусть дано следующее предложение:

 sentence6 = sp(u'compute computer computed computing') 

Можно найти корни всех слов с помощью лемматизации spaCy следующим образом:

 for word in sentence6: 
 print(word.text, word.lemma_) 

Результат выполнения сценария:

 compute compute 
 computer computer 
 computed compute 
 computing computing 

В отличие от стемминга, в котором корень, который был получен, «вычислялся», 
корни, которые были здесь получены, являются настоящими словами в словаре.

Лемматизация преобразует слова во второй или третьей формах в их варианты первой формы. 
Например:

 sentence7 = sp(u'A letter has been written, asking him to be released') 

 for word in sentence7: 
 print(word.text + ' ===>', word.lemma_) 

Вывод:

 A ===> a 
 letter ===> letter 
 has ===> have 
 been ===> be 
 written ===> write 
 , ===> , 
 asking ===> ask 
 him ===> -PRON- 
 to ===> to 
 be ===> be 
 released ===> release 

Из выходных данных видно, что слова во второй и третьей формах, 
такие как «написано», «выпущено» и т. д., были преобразованы в первую форму, 
то есть «запись» и «выпуск».

Заключение
Токенизация, стемминг и лемматизация - фундаментальные задачи обработки естественного языка. 
Здесь было показано, как можно выполнить токенизацию и лемматизацию с помощью библиотеки spaCy. 
Также было показано, как NLTK можно использовать для стемминга. 

--------------------------------------------------------------------------
"""


# ========================================================================

def WordNet(wnl):
    # single word lemmatization examples
    list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',
             'driving', 'died', 'tried', 'feet']

    for words in list1:
        print(words + " ---> " + wnl.lemmatize(words))

    # > kites ---> kite
    # > babies ---> baby
    # > dogs ---> dog
    # > flying ---> flying
    # > smiling ---> smiling
    # > driving ---> driving
    # > died ---> died
    # > tried ---> tried
    # > feet ---> foot
    # ========================================================================
    # sentence lemmatization examples ========================================
    string = 'the cat is sitting with the bats on the striped mat under many flying geese'
    print(string)

    # Converting string into tokens ==========================================
    list2 = nltk.word_tokenize(string)
    print(list2)
    # > ['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on',
    #   'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']

    lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])

    print(lemmatized_string)


# > the cat is sitting with the bat on the striped mat under many flying goose


def WordNetPos(wnl):
    lemmatizer = WordNetLemmatizer()

    # Define function to lemmatize each word with its POS tag

    # POS_TAGGER_FUNCTION : TYPE 1
    def pos_tagger(nltk_tag):
        if nltk_tag.startswith('J'):
            return wordnet.ADJ
        elif nltk_tag.startswith('V'):
            return wordnet.VERB
        elif nltk_tag.startswith('N'):
            return wordnet.NOUN
        elif nltk_tag.startswith('R'):
            return wordnet.ADV
        else:
            return None

    sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'

    # tokenize the sentence and find the POS tag for each token
    pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))

    print(pos_tagged)
    # >[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'),
    # ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'),
    # ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('flying', 'VBG'), ('geese', 'JJ')]

    # As you may have noticed, the above pos tags are a little confusing.

    # we use our own pos_tagger function to make things simpler to understand.
    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))
    print(wordnet_tagged)
    # >[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None),
    # ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'),
    # ('mat', 'n'), ('under', None), ('many', 'a'), ('flying', 'v'), ('geese', 'a')]

    lemmatized_sentence = []
    for word, tag in wordnet_tagged:
        if tag is None:
            # if there is no available tag, append the token as is
            lemmatized_sentence.append(word)
        else:
            # else use the tag to lemmatize the token
            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
    lemmatized_sentence = " ".join(lemmatized_sentence)

    print(lemmatized_sentence)
    # > the cat can be sit with the bat on the striped mat under many fly geese

# ========================================================================

def WordTextBlob(wnl):
    tstWord = 'cats'
    # create a Word object
    w = Word(tstWord)
    print(w.lemmatize())
    # > cat

    sentence = 'the bats saw the cats with stripes hanging upside down by their feet.'

    # сборка строки из результатов работы лемматизатора
    s = TextBlob(sentence)
    lemmatized_sentence = " ".join([w.lemmatize() for w in s.words])

    print(lemmatized_sentence)
    # > the bat saw the cat with stripe hanging upside down by their foot

# ========================================================================

def WordTextBlob(wnl):
    tstWord = 'cats'
    # create a Word object
    w = Word(tstWord)
    print(w.lemmatize())
    # > cat

    sentence = 'the bats saw the cats with stripes hanging upside down by their feet.'

    # сборка строки из результатов работы лемматизатора
    s = TextBlob(sentence)
    lemmatized_sentence = " ".join([w.lemmatize() for w in s.words])

    print(lemmatized_sentence)
    # > the bat saw the cat with stripe hanging upside down by their foot

# ========================================================================

def getWordNetPos(sentence):
    """
    Map POS tag to first character lemmatize() accepts
    Wordnet Lemmatizer с соответствующим POS-тегом
    Достаточно сложно вручную проставить соответствующий POS-тег для каждого слова
    при обработке больших текстов.
    Поэтому вместо этого находят правильный POS-тег для каждого слова,
    сопоставляют его с правильным входным символом, который принимает WordnetLemmatizer,
    и передают его в качестве второго аргумента в lemmatize().

    Как получить POS-тег для выбранного слова?
    В nltk для этого есть метод nltk.pos_tag().
    Он принимает только список (список слов), даже если нужно передать только одно слово.
    """
    tag = nltk.pos_tag([sentence])[0][1][0].upper()
    # nltk.pos_tag() возвращает кортеж с тегом POS.
    # здесь принципиально сопоставление POS-тегов NLTK с форматом,
    # принятым лемматизатором wordnet
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

def WordTextPos(wnl):
    tstWord = 'feet'
    # print(wnl.lemmatize(tstWord, getWordNetPos(tstWord)))
    # create a Word object
    w = Word(tstWord)
    print(w.lemmatize())

    # > the bat saw the cat with stripe hang upside down by their foot
    sentence = 'the bats saw the cats with stripes hanging upside down by their feet.'
    print(sentence)

    # resSentence = [wnl.lemmatize(w, getWordNetPos(w)) for w in nltk.word_tokenize(sentence)]
    #                               ^^^^^^^^^^^^^     правильный POS-тег для каждого слова в sentence
    #                               второй аргумент в lemmatize() - это результат getWordNetPos
    #                            правильный POS-тег для каждого слова,
    #                   lemmatize с двумя аргументами занимается СОПОСТАВЛЕНИЕМ
    #             WordnetLemmatizer принимает результат сопоставления и возвращает массив
    #  _________________________ И вся эта хрень в одну строку! ____________________________________

    # _____________ А вот то же самое в две строки. Легче стало? ___________________________________
    resSentence = []
    for w in nltk.word_tokenize(sentence):
        resSentence.append(wnl.lemmatize(w, getWordNetPos(w)))

    print(resSentence)
    # > the bat saw the cat with stripe hanging upside down by their foot

# ========================================================================

"""

# Install spaCy (run in terminal/prompt)
import sys

# Установка spaCy 
pip3 install -U spacy

# Загрузка модели для анализа английского языка
python3 -m spacy download en_core_web_lg

# Установка textacy
pip3 install -U textacy

spaCy по умолчанию определяет часть речи и назначает соответствующую лемму.

"""


def xspaCy():
    # Загрузка английской или русской NLP-модели

    # sp = spacy.load("ru_core_news_lg") # ??? грузится,но не работает
    # sp = spacy.load("en_core_web_sm")  # а так он анализирует как en, так и ru
    # sp = spacy.load("en_core_web_lg")  # а так он анализирует как en, так и ru
    # sp = spacy.load("en_core_web_trf")  # а так он анализирует ТОЛЬКО en

    sp = spacy.load("en_core_web_md")  # а так он анализирует как en, так и ru,
    # даже лучше, чем sm

    # используется функция load spacy библиотеки для загрузки базовой (возможно, английской)
    # языковой модели. Модель хранится в переменной sp
    # # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization

    # Текст для анализа
    text = """London is the capital and most populous city of England and 
    the United Kingdom.  Standing on the River Thames in the south east 
    of the island of Great Britain, London has been a major settlement 
    for two millennia. It was founded by the Romans, who named it Londinium.
    spaCy является относительно новым пакетом и на данный момент считается 
    стандартом в индустрии NLP is'nt. Он поставляется с предварительно созданными моделями, 
    которые могут анализировать текст и выполнять различный функционал, связанный с NLP.
    Прежде всего надо установить spaCy и загрузить модель «en».
    """

    # Парсинг текста с помощью spaCy. Эта команда запускает конвейер
    doc = sp(text)
    # в переменной 'doc' теперь содержится обработанная версия текста
    # когда документ создается с использованием модели.
    # SpaCy автоматически разбивает документ text на токены.
    # Токен относится к отдельной части предложения, имеющей некоторое семантическое значение.

    # Можно посмотреть, какие токены есть в документе.
    for word in doc:
        print(word.text)

    # можно распечатать все обнаруженные именованные сущности
    for entity in doc.ents:
        print(f"{entity.text} ({entity.label_})")

    # используя .pos_ показанный ниже, также можно посмотреть части речи каждого из этих токенов
    for word in doc:
        print(word.text, word.pos_)

    # видеть, что каждому слову или символу в нашем предложении была отведена часть речи.
    # Например, «London» был помечен как существительное собственное, «is» - как глагол
    # и так далее

    # Помимо частей речи, также можем видеть зависимости.
    for word in doc:
        print(word.text, word.pos_, word.dep_)

    # Из выходных данных видно, что spaCy может найти зависимость между токенами,
    # например, в предложении, которое у нас есть, есть слово is'nt .
    # Синтаксический анализатор зависимостей разбил его на два слова и указывает,
    # что n't на самом деле является отрицанием предыдущего слова.
    # Помимо печати слов, также можно распечатать предложения из документа.

    for sentence in doc.sents:
        print(sentence)

    # также можно проверить, начинается ли предложение с определенного токена или нет.
    # Вы можете получить отдельные токены, используя индекс и квадратные скобки, как массив:

    print(doc[5])

    # В приведенном выше сценарии мы ищется 6-е слово в документе.
    # Имейте в виду, что индекс начинается с нуля, а период считается токеном.
    # На выходе вы должны увидеть: populous

    # Теперь, чтобы увидеть, начинается ли какое-либо предложение в документе с
    # populous, можно использовать is_sent_start как показано ниже:

    if doc[5].is_sent_start == True:
        print("Yes!")
    else:
        print("No!")

    # токенизацию spaCy. С использованием следующего скрипта создаётся новый документ:

    doc = sp(u'"They\'re leaving UK for USA"')
    print(doc)

    # Предложение содержит кавычки в начале и в конце.
    # Оно также содержит знаки препинания в сокращениях «UK» и «USA».

    for word in doc:
        print(word.text)

    # В выходных данных можно видеть, что spaCy разметила начальную и
    # конечную двойные кавычки.
    # И он НЕ разметил точку пунктуации между аббревиатурами UK и USA.

    # Ещё один пример токенизации:

    doc = sp(u"Hello, I am non-vegetarian, email me the menu at [email protected] ")
    print(doc)

    for word in doc:
        print(word.text)

    # Как можно подсчитать слова в документе:

    print(len(doc))

    # Обнаружение сущностей
    # Помимо токенизации документов в слова, также можно узнать,
    # является ли слово сущностью, такой как компания, место, здание, валюта, учреждение и т. д.
    # Пример распознавания именованных сущностей:

    doc = sp(u'Manchester United is looking to sign Harry Kane for $90 million')

    # Простая токенизация:

    for word in doc:
        print(word.text)

    # Известно, что «Манчестер Юнайтед» - это одно слово, поэтому его нельзя
    # токенизировать двумя словами. Точно так же «Гарри Кейн» - это имя человека,
    # а «90 миллионов долларов» - это денежная ценность. Их также не следует токенизировать.
    # Здесь и применяется распознавание именованных сущностей.
    # Чтобы получить именованные сущности из документа, надо использовать атрибут ents.
    # Теперь извлечние названных сущностей из приведенного выше предложения.
    # Надо вВыполнить следующий скрипт:

    for word in doc.ents:
        print(word.text + ' - ' + word.label_ + ' - ' + str(spacy.explain(word.label_)))

    # В приведенном выше скрипте печатается текст объекта, метка объекта и детали объекта.
    # Средство распознавания именованных сущностей spaCy успешно распознало
    # «Манчестер Юнайтед» как организацию, «Гарри Кейна» как личность и
    # «90 миллионов долларов» как денежную ценность.

    # Обнаружение существительных
    # Помимо обнаружения именованных сущностей, также могут быть обнаружены
    # существительные. Для этого применяется атрибут noun_chunks:

    doc = sp(u'Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million')

    # найти существительные из этого предложения (существительное также может быть именованным объектом, и наоборот.):

    for word in doc.noun_chunks:
        print(word.text)

    # Стемминг
    # Основание относится к приведению слова к его корневой форме.
    # При выполнении задач обработки естественного языка вы столкнетесь с
    # различными сценариями, в которых можно найти разные слова с одним и тем же корнем.
    # Например, compute, computer, computing, computed и т. Д.
    # Можно уменьшить слова до их корневой формы для единообразия.
    # И здесь работает стемминг.
    # spaCy применяет только лемматизацию и в нём нет функции для стемминга.
    # Поэтому для стемминга используется NLTK.
    #
    # В NLTK есть два типа стеммеров: Porter Stemmer и Snowball.
    # Оба они были реализованы с использованием разных алгоритмов.
    #
    # Портер Стеммер
    #
    # import nltk  # !!!!!
    # from nltk.stem.porter import *
    # Создаётся класс PorterStemmer.

    stemmer = PorterStemmer()

    # Пусть имеется следующий список, и надо сократить эти слова до основы:

    tokens = ['compute', 'computer', 'computed', 'computing']

    # Следующий скрипт находит основу для слов в списке с помощью стеммера портера:

    for token in tokens:
        print(token + ' --> ' + stemmer.stem(token))

    # Результат выглядит следующим образом:
    #
    #  compute --> comput
    #  computer --> comput
    #  computed --> comput
    #  computing --> comput

    # Видно, что все 4 слова были сокращены до «вычислить», что на самом деле вовсе не слово.
    #
    # Снежок Стеммер
    # Стеммер Snowball - это немного улучшенная версия стеммера Porter,
    # которую обычно предпочитают последнему. Стеммер снежка в действии:
    #
    # from nltk.stem.snowball import SnowballStemmer

    stemmer = SnowballStemmer(language='english')

    tokens = ['compute', 'computer', 'computed', 'computing']

    for token in tokens:
        print(token + ' --> ' + stemmer.stem(token))

    # В приведенном сценарии используется стеммер Snowball,
    # чтобы найти основу тех же 4 слов, что и стеммер портера. Результат выглядит так:
    #
    #  compute --> comput
    #  computer --> comput
    #  computed --> comput
    #  computing --> comput

    # Можно видеть, что результаты такие же. Всё ещё есть «вычислитель» в качестве основы.
    # Опять же, слово «вычислить» на самом деле не словарное.
    #
    # Здесь нужна лемматизация. Лемматизация сокращает слово до основы,
    # как оно появляется в словаре. Основы, возвращаемые посредством лемматизации,
    # являются фактическими словарными словами и семантически полными,
    # в отличие от слов, возвращаемых стеммером.
    #
    # Лемматизация
    # Хотя и не возможно выполнить стемминг с помощью spaCy напрямую, всё же
    # можно выполнить эту лемматизацию с помощью spaCy.
    #
    # Для этого нам нужно использовать lemma_ атрибут на Spacy документа.
    # Пусть, есть следующее предложение:

    sentence6 = sp(u'compute computer computed computing')
    # Мы можем найти корни всех слов с помощью лемматизации spaCy следующим образом:

    for word in sentence6:
        print(word.text, word.lemma_)

    # Результат выполнения сценария выше выглядит следующим образом:
    #
    #  compute compute
    #  computer computer
    #  computed compute
    #  computing computing

    # Можно видеть, что в отличие от стемминга, в котором корень,
    # который мы получается, был «вычислить», корни, которые мы здесь получили,
    # являются настоящими словами в словаре.
    #
    # Лемматизация преобразует слова во второй или третьей формах в их варианты первой формы.
    # Ещё один пример:
    #
    sentence7 = sp(u'A letter has been written, asking him to be released')

    for word in sentence7:
        print(word.text + ' ===>', word.lemma_)

    # Вывод: ???
    #
    #  A ===> a
    #  letter ===> letter
    #  has ===> have
    #  been ===> be
    #  written ===> write
    #  , ===> ,
    #  asking ===> ask
    #  him ===> -PRON-
    #  to ===> to
    #  be ===> be
    #  released ===> release

    # Из выходных данных видно, что слова во второй и третьей формах,
    # такие как «написано», «выпущено» и т. Д.,
    # Были преобразованы в первую форму, то есть «запись» и «выпуск».
    #
    # Заключение
    # Токенизация, стемминг и лемматизация - одни из самых фундаментальных
    # задач обработки естественного языка. Здесь было показано,
    # как можно выполнить токенизацию и лемматизацию с помощью библиотеки spaCy.
    # Также было показано, как NLTK можно использовать для стемминга.

    # ========================================================================




#def Gensim():
    # sentence = "The striped bats were hanging on their feet and ate best fishes"
    # word = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]
    # print(word)
    # Gensim когда-либо ранее упаковывал подпрограммы лемматизации
    # # другой библиотеки (Pattern), что не было особенно современным или хорошо
    # # обслуживаемым вариантом, поэтому он был удален из Gensim-4.0.
    # # Пользователи могут выбирать и применять свои собственные операции лемматизации,
    # # если они считают это необходимым, в качестве этапа предварительной обработки
    # # перед применением алгоритмов Gensim.
    # #
    # # Некоторые библиотеки Python, предлагающие лемматизацию, включают:
    # # Шаблон (ранее включенная опция Gensim, которую можно использовать напрямую,
    # # а не через старую поддержку Gensim): https: // github .com / clips / pattern
    # # NLTK: https://www.nltk.org/ api / nltk.stem.html # nltk.stem.wordnet.WordNetLemmatizer
    # # UDPipe: https://ufal.mff.cuni.cz/udpipe
    # # Просторный: https://spacy.io/api/lemmatizer
    # # Станца: https://stanfordnlp.github.io/stanza/

# def TreeTagger():
#     tagger = ttpw.TreeTagger(TAGLANG='en', TAGDIR='c:\\users\\anton\\appdata\\roaming\\python\\python311\\site-packages')
#     tags = tagger.tag_text("The striped bats were hanging on their feet and ate best fishes")
#     lemmas = [t.split('\t')[-1] for t in tags]
# #     # > ['the', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'and', 'eat', 'good', 'fish']
#     print(lemmas)



def DoIt(name):  # Create WordNetLemmatizer object and use it

    print(name)

    wnl = WordNetLemmatizer()

    print(f'\n___{WordNet}___')
    WordNet(wnl)

    print(f'\n___{WordNetPos}___')
    WordNetPos(wnl)

    print(f'\n___{WordTextBlob}___')
    WordTextBlob(wnl)

    print(f'\n___{WordTextPos}___')
    WordTextPos(wnl)

    print(f'\n___{xspaCy}___')
    xspaCy()

    ## print(f'\n___{Gensim}___')
    ## Gensim()

    ## print(f'\n___{TreeTagger}___')
    ## TreeTagger()


def main(name):
    DoIt(name)


# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    main('PyCharm')
